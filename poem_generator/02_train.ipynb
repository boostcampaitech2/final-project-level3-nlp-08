{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, PreTrainedTokenizerFast\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['WANDB_WATCH'] = 'false'\n",
    "os.environ['WANDB_SILENT']=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# gpt2 base를 사용합니다.\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name, bos_token='<s>', \n",
    "                                                    eos_token='</s>', unk_token='<unk>',pad_token='<pad>', mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 스페셜 토큰 생성\n",
    "keyword_start_marker = '<k>'\n",
    "keyword_end_marker = '</k>'\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\n",
    "        keyword_start_marker,\n",
    "        keyword_end_marker\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer_config.json',\n",
       " './model/special_tokens_map.json',\n",
       " './model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스페셜 토큰이 새롭게 생긴 tokenizer를 저장합니다.\n",
    "tokenizer.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51202, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51202, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 스페셜 토큰만큼 모델 리사이즈\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(tokenizer.vocab_size + 2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword가 담겨있는 데이터를 로드\n",
    "poem_df = pd.read_csv('/opt/ml/data/final_poem.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, valid split\n",
    "train_poem, valid_poem = train_test_split(poem_df,test_size = 0.1, random_state=42)\n",
    "train_poem = train_poem.reset_index()\n",
    "valid_poem = valid_poem.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_emo_tag(df):\n",
    "    # <e> k1, k2, k3 </e> poem\n",
    "    text = df['text']\n",
    "    emo_tag = df['key_word']\n",
    "    tagged_text = keyword_start_marker + emo_tag + keyword_end_marker + text\n",
    "    return tagged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드 추출이 안돼서(명사 추출이 안돼서) keyword가 None인 경우가 존재\n",
    "# 그 경우 train, valid data에서 제외\n",
    "train_data = []\n",
    "valid_data = []\n",
    "\n",
    "for i in range(len(train_poem)):\n",
    "    try:\n",
    "        train_data.append(add_emo_tag(train_poem.iloc[i]))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "for i in range(len(valid_poem)):\n",
    "    try:\n",
    "        valid_data.append(add_emo_tag(valid_poem.iloc[i]))\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenizer(train_data, padding= True, return_tensors='pt')\n",
    "valid_data = tokenizer(valid_data, padding= True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids == labels\n",
    "# 생성모델이므로...\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self,data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    def __getitem__(self, index):\n",
    "        item = {k : v[index] for k, v in self.data.items()}\n",
    "        item['labels'] = self.data.input_ids[index]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.data.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoemDataset(train_data)\n",
    "valid_dataset = PoemDataset(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시의 bleu를 측정하는 것이 과연 옳은가?\n",
    "# loss 측정 하는게 시의 전체적인 확률 분포를 확인 하는 측면에서 더 맞는 접근이 아닌가?\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  \"\"\" validation을 위한 metrics function \"\"\"\n",
    "  labels = tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
    "  preds = tokenizer.batch_decode(pred.predictions, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "  # labels -> [sen1, sen2, sen3 ...]\n",
    "  # list_of_references -> [[sen1],[sen2],[sen3]...]\n",
    "  list_of_references = []\n",
    "  for i in range(len(labels)):\n",
    "    list_of_references.append([labels[i]])\n",
    "      \n",
    "  # calculate blue4\n",
    "  blue4 = corpus_bleu(list_of_references= list_of_references, hypotheses = preds)\n",
    "\n",
    "  return {\n",
    "      'blue4': blue4\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 26352\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 4110\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4110' max='4110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4110/4110 1:46:40, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.172100</td>\n",
       "      <td>1.100510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.738500</td>\n",
       "      <td>1.087253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.740200</td>\n",
       "      <td>1.077531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.743900</td>\n",
       "      <td>1.071910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.712200</td>\n",
       "      <td>1.073024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.688300</td>\n",
       "      <td>1.067213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.714400</td>\n",
       "      <td>1.063572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.706200</td>\n",
       "      <td>1.060801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>1.064001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.671700</td>\n",
       "      <td>1.061196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.679800</td>\n",
       "      <td>1.060837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>1.059044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.666600</td>\n",
       "      <td>1.063717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.657000</td>\n",
       "      <td>1.062277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.645400</td>\n",
       "      <td>1.061119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.058991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.645900</td>\n",
       "      <td>1.063204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>1.063813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>1.062118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.630200</td>\n",
       "      <td>1.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>1.070532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.617800</td>\n",
       "      <td>1.066615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>1.067481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.609900</td>\n",
       "      <td>1.065214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.613900</td>\n",
       "      <td>1.075227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.593600</td>\n",
       "      <td>1.070926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.598000</td>\n",
       "      <td>1.071895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.600100</td>\n",
       "      <td>1.071959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.607900</td>\n",
       "      <td>1.075768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>1.076134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.588100</td>\n",
       "      <td>1.074497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.584300</td>\n",
       "      <td>1.074589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.593300</td>\n",
       "      <td>1.079614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>1.078748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.579100</td>\n",
       "      <td>1.077412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>1.076894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.577700</td>\n",
       "      <td>1.076583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>1.080104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.569600</td>\n",
       "      <td>1.079871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>1.079603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.571600</td>\n",
       "      <td>1.079207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-100\n",
      "Configuration saved in ./model/checkpoint-100/config.json\n",
      "Model weights saved in ./model/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-100/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-200\n",
      "Configuration saved in ./model/checkpoint-200/config.json\n",
      "Model weights saved in ./model/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-300\n",
      "Configuration saved in ./model/checkpoint-300/config.json\n",
      "Model weights saved in ./model/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-400\n",
      "Configuration saved in ./model/checkpoint-400/config.json\n",
      "Model weights saved in ./model/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-500\n",
      "Configuration saved in ./model/checkpoint-500/config.json\n",
      "Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-600\n",
      "Configuration saved in ./model/checkpoint-600/config.json\n",
      "Model weights saved in ./model/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-700\n",
      "Configuration saved in ./model/checkpoint-700/config.json\n",
      "Model weights saved in ./model/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-800\n",
      "Configuration saved in ./model/checkpoint-800/config.json\n",
      "Model weights saved in ./model/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-900\n",
      "Configuration saved in ./model/checkpoint-900/config.json\n",
      "Model weights saved in ./model/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1000\n",
      "Configuration saved in ./model/checkpoint-1000/config.json\n",
      "Model weights saved in ./model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1100\n",
      "Configuration saved in ./model/checkpoint-1100/config.json\n",
      "Model weights saved in ./model/checkpoint-1100/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1100/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1100/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1200\n",
      "Configuration saved in ./model/checkpoint-1200/config.json\n",
      "Model weights saved in ./model/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1300\n",
      "Configuration saved in ./model/checkpoint-1300/config.json\n",
      "Model weights saved in ./model/checkpoint-1300/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1300/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1300/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1400\n",
      "Configuration saved in ./model/checkpoint-1400/config.json\n",
      "Model weights saved in ./model/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1500\n",
      "Configuration saved in ./model/checkpoint-1500/config.json\n",
      "Model weights saved in ./model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1600\n",
      "Configuration saved in ./model/checkpoint-1600/config.json\n",
      "Model weights saved in ./model/checkpoint-1600/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1700\n",
      "Configuration saved in ./model/checkpoint-1700/config.json\n",
      "Model weights saved in ./model/checkpoint-1700/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1700/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1700/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1800\n",
      "Configuration saved in ./model/checkpoint-1800/config.json\n",
      "Model weights saved in ./model/checkpoint-1800/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-1900\n",
      "Configuration saved in ./model/checkpoint-1900/config.json\n",
      "Model weights saved in ./model/checkpoint-1900/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-1900/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-1900/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2000\n",
      "Configuration saved in ./model/checkpoint-2000/config.json\n",
      "Model weights saved in ./model/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-1900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2100\n",
      "Configuration saved in ./model/checkpoint-2100/config.json\n",
      "Model weights saved in ./model/checkpoint-2100/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2100/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2100/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2200\n",
      "Configuration saved in ./model/checkpoint-2200/config.json\n",
      "Model weights saved in ./model/checkpoint-2200/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2300\n",
      "Configuration saved in ./model/checkpoint-2300/config.json\n",
      "Model weights saved in ./model/checkpoint-2300/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2300/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2300/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2400\n",
      "Configuration saved in ./model/checkpoint-2400/config.json\n",
      "Model weights saved in ./model/checkpoint-2400/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2400/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2500\n",
      "Configuration saved in ./model/checkpoint-2500/config.json\n",
      "Model weights saved in ./model/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2600\n",
      "Configuration saved in ./model/checkpoint-2600/config.json\n",
      "Model weights saved in ./model/checkpoint-2600/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2600/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2600/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2700\n",
      "Configuration saved in ./model/checkpoint-2700/config.json\n",
      "Model weights saved in ./model/checkpoint-2700/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2700/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2700/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2800\n",
      "Configuration saved in ./model/checkpoint-2800/config.json\n",
      "Model weights saved in ./model/checkpoint-2800/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2800/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-2900\n",
      "Configuration saved in ./model/checkpoint-2900/config.json\n",
      "Model weights saved in ./model/checkpoint-2900/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-2900/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-2900/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3000\n",
      "Configuration saved in ./model/checkpoint-3000/config.json\n",
      "Model weights saved in ./model/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-2900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3100\n",
      "Configuration saved in ./model/checkpoint-3100/config.json\n",
      "Model weights saved in ./model/checkpoint-3100/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3100/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3100/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3000] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3200\n",
      "Configuration saved in ./model/checkpoint-3200/config.json\n",
      "Model weights saved in ./model/checkpoint-3200/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3200/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3200/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3100] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3300\n",
      "Configuration saved in ./model/checkpoint-3300/config.json\n",
      "Model weights saved in ./model/checkpoint-3300/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3300/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3300/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3200] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3400\n",
      "Configuration saved in ./model/checkpoint-3400/config.json\n",
      "Model weights saved in ./model/checkpoint-3400/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3400/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3400/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3300] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3500\n",
      "Configuration saved in ./model/checkpoint-3500/config.json\n",
      "Model weights saved in ./model/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3400] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3600\n",
      "Configuration saved in ./model/checkpoint-3600/config.json\n",
      "Model weights saved in ./model/checkpoint-3600/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3600/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3600/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3500] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3700\n",
      "Configuration saved in ./model/checkpoint-3700/config.json\n",
      "Model weights saved in ./model/checkpoint-3700/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3700/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3700/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3600] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3800\n",
      "Configuration saved in ./model/checkpoint-3800/config.json\n",
      "Model weights saved in ./model/checkpoint-3800/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3800/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3800/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3700] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-3900\n",
      "Configuration saved in ./model/checkpoint-3900/config.json\n",
      "Model weights saved in ./model/checkpoint-3900/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-3900/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-3900/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3800] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-4000\n",
      "Configuration saved in ./model/checkpoint-4000/config.json\n",
      "Model weights saved in ./model/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-3900] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2926\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./model/checkpoint-4100\n",
      "Configuration saved in ./model/checkpoint-4100/config.json\n",
      "Model weights saved in ./model/checkpoint-4100/pytorch_model.bin\n",
      "tokenizer config file saved in ./model/checkpoint-4100/tokenizer_config.json\n",
      "Special tokens file saved in ./model/checkpoint-4100/special_tokens_map.json\n",
      "Deleting older checkpoint [model/checkpoint-4000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4110, training_loss=0.6465582943890796, metrics={'train_runtime': 6408.382, 'train_samples_per_second': 41.121, 'train_steps_per_second': 0.641, 'total_flos': 6.7229623296e+16, 'train_loss': 0.6465582943890796, 'epoch': 10.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    #predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=True,\n",
    "    #load_best_model_at_end=True,\n",
    "    output_dir='./model',\n",
    "    logging_steps=100,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    #compute_metrics = compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./model/config.json\n",
      "Model weights saved in ./model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
